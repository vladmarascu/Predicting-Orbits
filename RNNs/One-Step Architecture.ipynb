{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "featured-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "flush-nylon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x_vv</th>\n",
       "      <th>y_vv</th>\n",
       "      <th>z_vv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42159.783600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.074505</td>\n",
       "      <td>0.053666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39159.545914</td>\n",
       "      <td>15563.916303</td>\n",
       "      <td>266.913399</td>\n",
       "      <td>-1.142984</td>\n",
       "      <td>2.852848</td>\n",
       "      <td>0.048528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30572.229294</td>\n",
       "      <td>28909.053086</td>\n",
       "      <td>496.928223</td>\n",
       "      <td>-2.127756</td>\n",
       "      <td>2.228986</td>\n",
       "      <td>0.039518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17596.068412</td>\n",
       "      <td>38144.740319</td>\n",
       "      <td>671.356184</td>\n",
       "      <td>-2.813645</td>\n",
       "      <td>1.287415</td>\n",
       "      <td>0.027234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2068.446366</td>\n",
       "      <td>41949.706443</td>\n",
       "      <td>775.646554</td>\n",
       "      <td>-3.098444</td>\n",
       "      <td>0.161675</td>\n",
       "      <td>0.012698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>38132.961517</td>\n",
       "      <td>21370.880108</td>\n",
       "      <td>-7532.333000</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>2.199413</td>\n",
       "      <td>1.750026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>31399.515894</td>\n",
       "      <td>31233.289707</td>\n",
       "      <td>1828.575655</td>\n",
       "      <td>-1.692745</td>\n",
       "      <td>1.562118</td>\n",
       "      <td>1.820311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>20906.077447</td>\n",
       "      <td>37238.565267</td>\n",
       "      <td>10967.804062</td>\n",
       "      <td>-2.312211</td>\n",
       "      <td>0.726117</td>\n",
       "      <td>1.664753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>7871.245318</td>\n",
       "      <td>38575.122415</td>\n",
       "      <td>18731.818721</td>\n",
       "      <td>-2.661184</td>\n",
       "      <td>-0.221639</td>\n",
       "      <td>1.293445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-6142.526530</td>\n",
       "      <td>34919.272230</td>\n",
       "      <td>24066.202237</td>\n",
       "      <td>-2.681132</td>\n",
       "      <td>-1.179765</td>\n",
       "      <td>0.734721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x             y             z      x_vv      y_vv      z_vv\n",
       "0    42159.783600      0.000000      0.000000  0.000000  3.074505  0.053666\n",
       "1    39159.545914  15563.916303    266.913399 -1.142984  2.852848  0.048528\n",
       "2    30572.229294  28909.053086    496.928223 -2.127756  2.228986  0.039518\n",
       "3    17596.068412  38144.740319    671.356184 -2.813645  1.287415  0.027234\n",
       "4     2068.446366  41949.706443    775.646554 -3.098444  0.161675  0.012698\n",
       "..            ...           ...           ...       ...       ...       ...\n",
       "995  38132.961517  21370.880108  -7532.333000 -0.876644  2.199413  1.750026\n",
       "996  31399.515894  31233.289707   1828.575655 -1.692745  1.562118  1.820311\n",
       "997  20906.077447  37238.565267  10967.804062 -2.312211  0.726117  1.664753\n",
       "998   7871.245318  38575.122415  18731.818721 -2.661184 -0.221639  1.293445\n",
       "999  -6142.526530  34919.272230  24066.202237 -2.681132 -1.179765  0.734721\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load DataFrame and Libs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#sns.set_style('whitegrid')\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "df = pd.read_csv('Dataframes/3rdBodyPerturbationPoliastro.csv')\n",
    "df_v = pd.read_csv('Dataframes/3rdBodyPerturbationPoliastro_vv.csv')\n",
    "\n",
    "df['x_vv']=df_v.vv_x\n",
    "df['y_vv']=df_v.vv_y\n",
    "df['z_vv']=df_v.vv_z\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "explicit-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "train_size = int(len(df) * 0.8) \n",
    "\n",
    "train_df,test_df = df[1:train_size], df[train_size:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "corporate-demonstration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x_vv</th>\n",
       "      <th>y_vv</th>\n",
       "      <th>z_vv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>-38640.279625</td>\n",
       "      <td>-2764.935183</td>\n",
       "      <td>17604.980941</td>\n",
       "      <td>-0.242379</td>\n",
       "      <td>-2.869481</td>\n",
       "      <td>-1.032661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>-37222.004154</td>\n",
       "      <td>-17134.306183</td>\n",
       "      <td>11135.984105</td>\n",
       "      <td>0.783117</td>\n",
       "      <td>-2.603604</td>\n",
       "      <td>-1.431914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>-30699.416577</td>\n",
       "      <td>-29154.049729</td>\n",
       "      <td>3087.546466</td>\n",
       "      <td>1.701144</td>\n",
       "      <td>-1.974296</td>\n",
       "      <td>-1.633559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>-19981.800403</td>\n",
       "      <td>-37148.488205</td>\n",
       "      <td>-5420.909441</td>\n",
       "      <td>2.380643</td>\n",
       "      <td>-1.071344</td>\n",
       "      <td>-1.607049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>-6568.003292</td>\n",
       "      <td>-40025.406603</td>\n",
       "      <td>-13203.492438</td>\n",
       "      <td>2.729043</td>\n",
       "      <td>-0.025797</td>\n",
       "      <td>-1.357603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>38132.961517</td>\n",
       "      <td>21370.880108</td>\n",
       "      <td>-7532.333000</td>\n",
       "      <td>-0.876644</td>\n",
       "      <td>2.199413</td>\n",
       "      <td>1.750026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>31399.515894</td>\n",
       "      <td>31233.289707</td>\n",
       "      <td>1828.575655</td>\n",
       "      <td>-1.692745</td>\n",
       "      <td>1.562118</td>\n",
       "      <td>1.820311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>20906.077447</td>\n",
       "      <td>37238.565267</td>\n",
       "      <td>10967.804062</td>\n",
       "      <td>-2.312211</td>\n",
       "      <td>0.726117</td>\n",
       "      <td>1.664753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>7871.245318</td>\n",
       "      <td>38575.122415</td>\n",
       "      <td>18731.818721</td>\n",
       "      <td>-2.661184</td>\n",
       "      <td>-0.221639</td>\n",
       "      <td>1.293445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-6142.526530</td>\n",
       "      <td>34919.272230</td>\n",
       "      <td>24066.202237</td>\n",
       "      <td>-2.681132</td>\n",
       "      <td>-1.179765</td>\n",
       "      <td>0.734721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x             y             z      x_vv      y_vv      z_vv\n",
       "800 -38640.279625  -2764.935183  17604.980941 -0.242379 -2.869481 -1.032661\n",
       "801 -37222.004154 -17134.306183  11135.984105  0.783117 -2.603604 -1.431914\n",
       "802 -30699.416577 -29154.049729   3087.546466  1.701144 -1.974296 -1.633559\n",
       "803 -19981.800403 -37148.488205  -5420.909441  2.380643 -1.071344 -1.607049\n",
       "804  -6568.003292 -40025.406603 -13203.492438  2.729043 -0.025797 -1.357603\n",
       "..            ...           ...           ...       ...       ...       ...\n",
       "995  38132.961517  21370.880108  -7532.333000 -0.876644  2.199413  1.750026\n",
       "996  31399.515894  31233.289707   1828.575655 -1.692745  1.562118  1.820311\n",
       "997  20906.077447  37238.565267  10967.804062 -2.312211  0.726117  1.664753\n",
       "998   7871.245318  38575.122415  18731.818721 -2.661184 -0.221639  1.293445\n",
       "999  -6142.526530  34919.272230  24066.202237 -2.681132 -1.179765  0.734721\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neither-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "\n",
    "train = train_df\n",
    "scalers={}\n",
    "for i in train_df.columns:\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    s_s = scaler.fit_transform(train[i].values.reshape(-1,1))\n",
    "    s_s=np.reshape(s_s,len(s_s))\n",
    "    scalers['scaler_'+ i] = scaler\n",
    "    train[i]=s_s\n",
    "test = test_df\n",
    "for i in train_df.columns:\n",
    "    scaler = scalers['scaler_'+i]\n",
    "    s_s = scaler.transform(test[i].values.reshape(-1,1))\n",
    "    s_s=np.reshape(s_s,len(s_s))\n",
    "    scalers['scaler_'+i] = scaler\n",
    "    test[i]=s_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stupid-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will make a function that will use a sliding window approach to transform our series into samples \n",
    "# of input past observations and output future observations to use supervised learning algorithms.\n",
    "\n",
    "# FUNCTION\n",
    "def split_series(series, n_past, n_future):\n",
    "    # n_past ==> no of past observations\n",
    "    # n_future ==> no of future observations \n",
    "    X, y = list(), list()\n",
    "    for window_start in range(len(series)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(series):\n",
    "            break\n",
    "        # slicing the past and future parts of the window\n",
    "        past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
    "        X.append(past)\n",
    "        y.append(future)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "limited-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and test data:\n",
    "\n",
    "# Variables\n",
    "n_past = 60\n",
    "n_future = 5\n",
    "n_features = 6\n",
    "\n",
    "# CALLS\n",
    "X_train, y_train = split_series(train.values, n_past, n_future)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], n_features))\n",
    "\n",
    "X_test, y_test = split_series(test.values, n_past, n_future)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "white-macedonia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 60, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "revolutionary-absolute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 60, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rolled-package",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 5, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sought-consideration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 5, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-headline",
   "metadata": {},
   "source": [
    "# Multivariate model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "tested-workshop",
   "metadata": {},
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "# Build the LSTM model\n",
    "# Note: replace LSTM with GRU or RNN if you want to try those\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape= (n_past, n_features)))\n",
    "model.add(Dense(n_features))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history=model.fit(X_train, y_train, batch_size=256, epochs=100, validation_data=(X_test,y_test))\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.plot(history.history['loss']) # add validation loss\n",
    "plt.title('model loss')\n",
    "plt.ylabel('period')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "alone-communication",
   "metadata": {},
   "source": [
    "pred = model.predict(X_test)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-vacuum",
   "metadata": {},
   "source": [
    "# Model 1: E1D1 ==> Sequence to Sequence Model with one encoder layer and one decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "responsible-grenada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3/3 [==============================] - 1s 400ms/step - loss: 0.1553 - val_loss: 0.1599\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.1066 - val_loss: 0.1050\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.0625 - val_loss: 0.0660\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.0402 - val_loss: 0.0675\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.0427 - val_loss: 0.0556\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.0297 - val_loss: 0.0400\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.0209 - val_loss: 0.0357\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 0.0184 - val_loss: 0.0317\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.0141 - val_loss: 0.0263\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.0099 - val_loss: 0.0250\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.0090 - val_loss: 0.0241\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.0077 - val_loss: 0.0209\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.0065 - val_loss: 0.0196\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.0066 - val_loss: 0.0182\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.0059 - val_loss: 0.0166\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.0052 - val_loss: 0.0159\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 0.0049 - val_loss: 0.0139\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.0041 - val_loss: 0.0113\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.0038 - val_loss: 0.0095\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.0034 - val_loss: 0.0080\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.0029 - val_loss: 0.0069\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.0026 - val_loss: 0.0058\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 1s 357ms/step - loss: 0.0024 - val_loss: 0.0052\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 1s 359ms/step - loss: 0.0022 - val_loss: 0.0049\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 1s 332ms/step - loss: 0.0021 - val_loss: 0.0047\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 0.0020 - val_loss: 0.0045\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 1s 432ms/step - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 1s 352ms/step - loss: 0.0018 - val_loss: 0.0040\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 1s 341ms/step - loss: 0.0017 - val_loss: 0.0038\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 1s 334ms/step - loss: 0.0016 - val_loss: 0.0036\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 1s 343ms/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 1s 370ms/step - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 1s 354ms/step - loss: 0.0014 - val_loss: 0.0029\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 1s 345ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0012 - val_loss: 0.0024\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 0.0011 - val_loss: 0.0021\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 9.9227e-04 - val_loss: 0.0018\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 9.3673e-04 - val_loss: 0.0018\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 8.8165e-04 - val_loss: 0.0017\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 157ms/step - loss: 8.2683e-04 - val_loss: 0.0016\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 7.7829e-04 - val_loss: 0.0015\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 1s 303ms/step - loss: 7.3104e-04 - val_loss: 0.0014\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 6.8920e-04 - val_loss: 0.0013\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 6.5618e-04 - val_loss: 0.0013\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 6.1824e-04 - val_loss: 0.0013\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 5.8878e-04 - val_loss: 0.0013\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 1s 175ms/step - loss: 5.5888e-04 - val_loss: 0.0013\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 1s 173ms/step - loss: 5.3645e-04 - val_loss: 0.0013\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 131ms/step - loss: 5.1190e-04 - val_loss: 0.0013\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 4.9089e-04 - val_loss: 0.0014\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 4.7289e-04 - val_loss: 0.0013\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 4.5969e-04 - val_loss: 0.0013\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 4.4387e-04 - val_loss: 0.0014\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 4.3003e-04 - val_loss: 0.0014\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 4.1303e-04 - val_loss: 0.0012\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 4.0028e-04 - val_loss: 0.0012\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 3.8431e-04 - val_loss: 0.0012\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 3.7507e-04 - val_loss: 0.0012\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 3.6061e-04 - val_loss: 0.0010\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 160ms/step - loss: 3.4802e-04 - val_loss: 9.9224e-04\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 3.3845e-04 - val_loss: 0.0010\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 3.2201e-04 - val_loss: 0.0010\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 3.1106e-04 - val_loss: 8.6899e-04\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 3.0208e-04 - val_loss: 0.0011\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 3.0866e-04 - val_loss: 8.7318e-04\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 2.8744e-04 - val_loss: 8.0613e-04\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 2.8361e-04 - val_loss: 7.8085e-04\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 2.6770e-04 - val_loss: 8.2274e-04\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 2.6238e-04 - val_loss: 7.3896e-04\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 2.6115e-04 - val_loss: 7.7446e-04\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 2.5259e-04 - val_loss: 7.1849e-04\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 2.6136e-04 - val_loss: 7.6079e-04\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 2.5468e-04 - val_loss: 6.8771e-04\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 2.4293e-04 - val_loss: 6.9250e-04\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 2.3610e-04 - val_loss: 6.9377e-04\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 2.3509e-04 - val_loss: 7.1068e-04\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 2.2853e-04 - val_loss: 6.6683e-04\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 2.2641e-04 - val_loss: 6.6926e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 2.2334e-04 - val_loss: 6.3242e-04\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 2.1977e-04 - val_loss: 6.0777e-04\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 2.2912e-04 - val_loss: 6.1254e-04\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 2.2923e-04 - val_loss: 6.2922e-04\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 2.2601e-04 - val_loss: 6.0383e-04\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 2.0998e-04 - val_loss: 6.2290e-04\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 2.0794e-04 - val_loss: 5.6625e-04\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 2.0289e-04 - val_loss: 5.8493e-04\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 1.9941e-04 - val_loss: 5.7159e-04\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 1.9917e-04 - val_loss: 5.6870e-04\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 1.9652e-04 - val_loss: 5.8088e-04\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 1.9596e-04 - val_loss: 5.4512e-04\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 1.9396e-04 - val_loss: 5.5595e-04\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 1.9446e-04 - val_loss: 5.7696e-04\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 1.9284e-04 - val_loss: 5.6883e-04\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 1.9534e-04 - val_loss: 5.8257e-04\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 2.0281e-04 - val_loss: 5.4079e-04\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 1.9812e-04 - val_loss: 5.4171e-04\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 1.9784e-04 - val_loss: 4.9530e-04\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 129ms/step - loss: 1.8754e-04 - val_loss: 5.4703e-04\n"
     ]
    }
   ],
   "source": [
    "# E1D1\n",
    "# n_features ==> no of features at each timestep in the data.\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
    "encoder_l1 = tf.keras.layers.LSTM(128, return_state=True)\n",
    "encoder_outputs1 = encoder_l1(encoder_inputs)\n",
    "encoder_states1 = encoder_outputs1[1:]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.RepeatVector(n_future)(encoder_outputs1[0])\n",
    "decoder_l1 = tf.keras.layers.LSTM(128, return_sequences=True)(decoder_inputs,initial_state = encoder_states1)\n",
    "decoder_outputs1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l1)\n",
    "\n",
    "model = tf.keras.models.Model(encoder_inputs,decoder_outputs1)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss = tf.keras.losses.Huber())\n",
    "\n",
    "history = model.fit(\n",
    "                    X_train, y_train, \n",
    "                    batch_size=256, \n",
    "                    epochs=100, \n",
    "                    validation_data=(X_test,y_test)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "smaller-undergraduate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 5, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hazardous-demonstration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 5, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index,i in enumerate(train_df.columns):\n",
    "    scaler = scalers['scaler_'+i]\n",
    "    pred[:,:,index]=scaler.inverse_transform(pred[:,:,index])\n",
    "    #y_train[:,:,index]=scaler.inverse_transform(y_train[:,:,index])\n",
    "    y_test[:,:,index]=scaler.inverse_transform(y_test[:,:,index])\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "supposed-difference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8580.68636836, -21580.77033154, -31508.40559114, -37025.09594659,\n",
       "       -37423.65401184, -32679.21080377, -23442.55060601, -10978.72035769,\n",
       "         2985.7054782 ,  16520.84509467,  27798.55840532,  35344.6529504 ,\n",
       "        38188.59658665,  35937.84933107,  28822.74069404,  17720.63777403,\n",
       "         4114.25199323, -10076.80861617, -22801.97239595, -32242.02806097,\n",
       "       -37102.74134816, -36769.24757745, -31336.97455908, -21571.53138319,\n",
       "        -8818.30767242,   5160.81489587,  18443.01905644,  29245.27551094,\n",
       "        36173.05174199,  38358.08582002,  35508.90974646,  27930.06425218,\n",
       "        16532.92597256,   2803.99330865, -11337.25085958, -23827.15132184,\n",
       "       -32834.79545718, -37090.67245283, -36060.64730559, -29961.60688874,\n",
       "       -19682.2591247 ,  -6654.86141998,   7325.09001583,  20349.69801356,\n",
       "        30687.39019716,  37022.46297369,  38582.7040899 ,  35170.55413207,\n",
       "        27159.65660567,  15491.25244685,   1649.62778429, -12452.37006549,\n",
       "       -24737.71141331, -33350.77554458, -37028.69713781, -35308.17355376,\n",
       "       -28532.91935015, -17728.54239601,  -4426.28091299,   9543.24348593,\n",
       "        22294.85496956,  32156.87124941,  37894.81342405,  38828.52610245,\n",
       "        34851.03435358,  26405.33305052,  14466.34054544,    517.42857641,\n",
       "       -13539.88205554, -25619.72478396, -33838.60403712, -36927.9100756 ,\n",
       "       -34489.60482525, -27001.73448062, -15644.79923912,  -2063.37073903,\n",
       "        11873.26032806,  24311.13151006,  33650.89771167,  38746.23568099,\n",
       "        39010.58053471,  34427.90365417,  25516.64921786,  13295.13031848,\n",
       "         -746.5769663 , -14720.80277592, -26546.1768939 , -34320.64602694,\n",
       "       -36771.32526512, -33562.55461185, -25314.78073976, -13378.36531582,\n",
       "          475.29130877,  14335.91691174,  26389.87912069,  35125.07291216,\n",
       "        39495.89966243,  39015.67441762,  33764.50643583,  24344.91238622,\n",
       "        11831.95399561,  -2267.16755835, -16081.65013268, -27554.21814001,\n",
       "       -34791.90830008, -36531.1656519 , -32500.5622256 , -23462.34321955,\n",
       "       -10942.64330403,   3154.32217645,  16877.11788193,  28460.65160328,\n",
       "        36494.1269554 ,  40046.84327523,  38741.77660214,  32762.22012687,\n",
       "        22803.73716024,  10009.9411011 ,  -4086.0420777 , -17635.30604664,\n",
       "       -28631.54042649, -35228.94677626, -36200.39125888, -31337.15373535,\n",
       "       -21527.83703048,  -8462.17081187,   5826.95157195,  19347.86748109,\n",
       "        30387.40542448,  37643.90463917,  40311.82768611,  38132.9615171 ,\n",
       "        31399.51589361,  20906.07744675,   7871.24531826,  -6142.52653033])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "warming-frost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7310.2075, -20416.328 , -30638.822 , -36454.66  , -37127.992 ,\n",
       "       -32454.846 , -22901.484 , -10072.177 ,   3721.1104,  16675.24  ,\n",
       "        27673.438 ,  35500.168 ,  38706.43  ,  36395.676 ,  29030.355 ,\n",
       "        18041.791 ,   4885.9995,  -8964.792 , -21714.18  , -31359.957 ,\n",
       "       -36495.75  , -36452.727 , -31070.967 , -20919.717 ,  -7772.1304,\n",
       "         6024.104 ,  18757.268 ,  29344.918 ,  36537.984 ,  38941.195 ,\n",
       "        35846.695 ,  27904.434 ,  16573.69  ,   3286.797 , -10467.148 ,\n",
       "       -22863.1   , -31948.688 , -36425.164 , -35683.117 , -29617.576 ,\n",
       "       -18914.146 ,  -5490.675 ,   8298.477 ,  20823.068 ,  31008.55  ,\n",
       "        37572.01  ,  39184.23  ,  35333.434 ,  26842.521 ,  15181.983 ,\n",
       "         1765.6252, -11892.879 , -23933.705 , -32460.344 , -36273.35  ,\n",
       "       -34820.113 , -28070.35  , -16847.836 ,  -3193.335 ,  10565.612 ,\n",
       "        22879.076 ,  32659.928 ,  38587.47  ,  39402.707 ,  34798.758 ,\n",
       "        25765.58  ,  13776.235 ,    230.6917, -13326.247 , -24993.416 ,\n",
       "       -32937.508 , -36051.47  , -33843.18  , -26386.611 , -14676.347 ,\n",
       "         -853.1996,  12826.871 ,  24903.57  ,  34259.785 ,  39527.957 ,\n",
       "        39520.203 ,  34149.97  ,  24574.428 ,  12262.961 ,  -1397.6271,\n",
       "       -14827.015 , -26078.127 , -33390.754 , -35743.188 , -32715.303 ,\n",
       "       -24525.598 , -12377.686 ,   1521.2302,  15044.461 ,  26834.38  ,\n",
       "        35727.13  ,  40296.637 ,  39430.266 ,  33285.684 ,  23184.898 ,\n",
       "        10583.569 ,  -3151.6924, -16402.285 , -27176.213 , -33796.32  ,\n",
       "       -35316.88  , -31410.07  , -22487.51  ,  -9990.77  ,   3861.4265,\n",
       "        17132.562 ,  28571.164 ,  36951.24  ,  40779.113 ,  39031.492 ,\n",
       "        32136.316 ,  21564.312 ,   8739.703 ,  -5004.8345, -18008.537 ,\n",
       "       -28244.184 , -34122.402 , -34762.34  , -29955.156 , -20353.738 ,\n",
       "        -7636.1387,   6040.889 ,  18973.684 ,  30000.549 ,  37823.76  ,\n",
       "        40882.45  ,  38268.105 ,  30692.426 ,  19738.768 ,   6777.7686,\n",
       "        -6903.089 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:,-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "individual-contact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: x\n",
      "Step < 0 > RMSE:  741.8636113655941\n",
      "\n",
      "Step < 1 > RMSE:  1461.105774440013\n",
      "\n",
      "Step < 2 > RMSE:  1171.3283051727487\n",
      "\n",
      "Step < 3 > RMSE:  930.4651206839704\n",
      "\n",
      "Step < 4 > RMSE:  891.343571907061\n",
      "\n",
      "Feature: y\n",
      "Step < 0 > RMSE:  1687.772864697907\n",
      "\n",
      "Step < 1 > RMSE:  843.3998473949282\n",
      "\n",
      "Step < 2 > RMSE:  1406.354374214958\n",
      "\n",
      "Step < 3 > RMSE:  1217.5692903208433\n",
      "\n",
      "Step < 4 > RMSE:  1140.4338398821203\n",
      "\n",
      "Feature: z\n",
      "Step < 0 > RMSE:  1271.1799591458705\n",
      "\n",
      "Step < 1 > RMSE:  1189.3627613345889\n",
      "\n",
      "Step < 2 > RMSE:  916.7499154299568\n",
      "\n",
      "Step < 3 > RMSE:  940.7466974957858\n",
      "\n",
      "Step < 4 > RMSE:  989.4400842412625\n",
      "\n",
      "Feature: x_vv\n",
      "Step < 0 > RMSE:  0.06431069453822269\n",
      "\n",
      "Step < 1 > RMSE:  0.09334642694541788\n",
      "\n",
      "Step < 2 > RMSE:  0.07568697992387693\n",
      "\n",
      "Step < 3 > RMSE:  0.06292630134831834\n",
      "\n",
      "Step < 4 > RMSE:  0.07033954918377133\n",
      "\n",
      "Feature: y_vv\n",
      "Step < 0 > RMSE:  0.08058858679974493\n",
      "\n",
      "Step < 1 > RMSE:  0.07054734172955823\n",
      "\n",
      "Step < 2 > RMSE:  0.09585313620611784\n",
      "\n",
      "Step < 3 > RMSE:  0.08707497089310015\n",
      "\n",
      "Step < 4 > RMSE:  0.07443634835396076\n",
      "\n",
      "Feature: z_vv\n",
      "Step < 0 > RMSE:  0.07230174143679026\n",
      "\n",
      "Step < 1 > RMSE:  0.06666658200697005\n",
      "\n",
      "Step < 2 > RMSE:  0.06161498240215932\n",
      "\n",
      "Step < 3 > RMSE:  0.0613706773089991\n",
      "\n",
      "Step < 4 > RMSE:  0.05570566899268839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,i in enumerate(train_df.columns):\n",
    "    #print(index)\n",
    "    print('Feature: '+str(i))\n",
    "    for j in range(0,n_future):\n",
    "        print(\"Step <\",j,\"> RMSE: \",math.sqrt(mean_squared_error(y_test[:,j-1,index],pred[:,j-1,index])))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-notice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
